{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpyCyi6msMcp",
        "outputId": "32ec57e3-4d16-43b4-f5e3-0520efbe77f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Major-project'...\n",
            "remote: Enumerating objects: 5731, done.\u001b[K\n",
            "remote: Counting objects: 100% (472/472), done.\u001b[K\n",
            "remote: Compressing objects: 100% (470/470), done.\u001b[K\n",
            "remote: Total 5731 (delta 3), reused 440 (delta 2), pack-reused 5259\u001b[K\n",
            "Receiving objects: 100% (5731/5731), 665.01 MiB | 20.32 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Updating files: 100% (5636/5636), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nabin25/Major-project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcbJ9SZts7hb",
        "outputId": "c0121613-48a6-45c1-d8bd-945181482d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amYRWoC0tHI8",
        "outputId": "9a750e07-98f0-427b-813c-e86b578efdab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 21 14:47:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "!pip install opencv-python\n",
        "!pip install mediapipe\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My3YgnbdtSHP",
        "outputId": "2549060a-7762-4abd-c074-e945c88d9555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.8 sounddevice-0.4.6\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "#path for image dataset \"Ba\"\n",
        "dataset_dir =\"Major-project/pre-images/pre-for-ba\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZzYqb3tt2jG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38054293-df5d-4925-d5ee-a75504ff2c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 96\n",
            "Validation set size: 32\n",
            "Test set size: 33\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0133.png', 'hand_0052.png', 'hand_0040.png', 'hand_0051.png', 'hand_0036.png']\n",
            "Validation set: ['hand_0115.png', 'hand_0079.png', 'hand_0113.png', 'hand_0055.png', 'hand_0081.png']\n",
            "Testing set: ['hand_0011.png', 'hand_0060.png', 'hand_0044.png', 'hand_0028.png', 'hand_0066.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bha"
      ],
      "metadata": {
        "id": "6BM3V1L-bMLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Bha\"\n",
        "dataset_dir =\"Major-project/pre-images/pre-for-bha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee_mEpjgbWPx",
        "outputId": "5fc21bc3-67de-4cfb-9f32-c5712114829e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 93\n",
            "Validation set size: 32\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0054.png', 'hand_0137.png', 'hand_0111.png', 'hand_0021.png', 'hand_0119.png']\n",
            "Validation set: ['hand_0039.png', 'hand_0062.png', 'hand_0098.png', 'hand_0038.png', 'hand_0044.png']\n",
            "Testing set: ['hand_0087.png', 'hand_0104.png', 'hand_0022.png', 'hand_0023.png', 'hand_0067.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cha"
      ],
      "metadata": {
        "id": "wO36NA3ybwLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"cha\"\n",
        "dataset_dir =\"Major-project/pre-images/pre-for-cha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIuk_hHObxv1",
        "outputId": "dd2f0d04-6f3f-41fe-aafe-e86085a7009a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 101\n",
            "Validation set size: 34\n",
            "Test set size: 34\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0120.png', 'hand_0149.png', 'hand_0144.png', 'hand_0031.png', 'hand_0008.png']\n",
            "Validation set: ['hand_0080.png', 'hand_0093.png', 'hand_0106.png', 'hand_0085.png', 'hand_0140.png']\n",
            "Testing set: ['hand_0102.png', 'hand_0110.png', 'hand_0023.png', 'hand_0078.png', 'hand_0109.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chha\n"
      ],
      "metadata": {
        "id": "98p_c61lb916"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"chha\"\n",
        "dataset_dir =\"Major-project/pre-images/pre-for-chha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvS78-hOb_6r",
        "outputId": "13a3b1b0-299a-41ee-9dad-8d39ffb5d82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 105\n",
            "Validation set size: 35\n",
            "Test set size: 35\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0028.png', 'hand_0106.png', 'hand_0153.png', 'hand_0132.png', 'hand_0104.png']\n",
            "Validation set: ['hand_0005.png', 'hand_0074.png', 'hand_0117.png', 'hand_0140.png', 'hand_0109.png']\n",
            "Testing set: ['hand_0027.png', 'hand_0045.png', 'hand_0160.png', 'hand_0147.png', 'hand_0003.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d-sha"
      ],
      "metadata": {
        "id": "E0OhW10OcHIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qoezpxmv3HMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"d-sha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-d_sha-स\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw8YA4itcLlG",
        "outputId": "e03dcd72-b692-40e1-e090-44c1bc10f52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 84\n",
            "Validation set size: 28\n",
            "Test set size: 28\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0065.png', 'hand_0069.png', 'hand_0139.png', 'hand_0115.png', 'hand_0114.png']\n",
            "Validation set: ['hand_0020.png', 'hand_0076.png', 'hand_0120.png', 'hand_0078.png', 'hand_0009.png']\n",
            "Testing set: ['hand_0047.png', 'hand_0014.png', 'hand_0097.png', 'hand_0027.png', 'hand_0018.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da\n"
      ],
      "metadata": {
        "id": "D139JtZechjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Da\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-da-ड\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAVTy7W8ckir",
        "outputId": "c5017b24-7415-46ad-f20a-f2b38f1a0b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 111\n",
            "Validation set size: 38\n",
            "Test set size: 38\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0182.png', 'hand_0033.png', 'hand_0086.png', 'hand_0012.png', 'hand_0119.png']\n",
            "Validation set: ['hand_0161.png', 'hand_0167.png', 'hand_0156.png', 'hand_0075.png', 'hand_0019.png']\n",
            "Testing set: ['hand_0025.png', 'hand_0072.png', 'hand_0040.png', 'hand_0023.png', 'hand_0129.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daa"
      ],
      "metadata": {
        "id": "eRGIb7zMCG8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Daa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-daa-द\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlh0ybrqCWJy",
        "outputId": "ae9dfed1-e68a-49a5-fc8f-38fd34d725e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 22\n",
            "Validation set size: 8\n",
            "Test set size: 8\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0043.png', 'hand_0105.png', 'hand_0008.png', 'hand_0075.png', 'hand_0101.png']\n",
            "Validation set: ['hand_0151.png', 'hand_0051.png', 'hand_0069.png', 'hand_0010.png', 'hand_0084.png']\n",
            "Testing set: ['hand_0183.png', 'hand_0089.png', 'hand_0050.png', 'hand_0121.png', 'hand_0164.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dha"
      ],
      "metadata": {
        "id": "YhtNkuMMChW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Dha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-dha-ढ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuW9qu9jCpNF",
        "outputId": "d604fd96-a45a-4a8e-9598-02599dea4e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 120\n",
            "Validation set size: 40\n",
            "Test set size: 40\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0027.png', 'hand_0126.png', 'hand_0057.png', 'hand_0111.png', 'hand_0184.png']\n",
            "Validation set: ['hand_0137.png', 'hand_0051.png', 'hand_0046.png', 'hand_0089.png', 'hand_0059.png']\n",
            "Testing set: ['hand_0079.png', 'hand_0065.png', 'hand_0055.png', 'hand_0061.png', 'hand_0122.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dhaa"
      ],
      "metadata": {
        "id": "n_IMO_aDC6e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Dhaa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-dhaa-ध\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "66xq4fHzC9Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ga\n"
      ],
      "metadata": {
        "id": "4VYXpsr7DGMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ga\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ga\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "DOf-5C-bDIDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be801f8f-f9e1-47d0-a709-08f08d8ab69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 72\n",
            "Validation set size: 24\n",
            "Test set size: 25\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0037.png', 'hand_0039.png', 'hand_0090.png', 'hand_0089.png', 'hand_0118.png']\n",
            "Validation set: ['hand_0021.png', 'hand_0101.png', 'hand_0061.png', 'hand_0111.png', 'hand_0048.png']\n",
            "Testing set: ['hand_0049.png', 'hand_0112.png', 'hand_0026.png', 'hand_0010.png', 'hand_0035.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gha"
      ],
      "metadata": {
        "id": "ESP0_rdIDJVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"gha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-gha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "axtlUMuhDKSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1079b7da-1167-4615-a6d3-944978d15a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 107\n",
            "Validation set size: 36\n",
            "Test set size: 36\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0119.png', 'hand_0161.png', 'hand_0067.png', 'hand_0159.png', 'hand_0096.png']\n",
            "Validation set: ['hand_0136.png', 'hand_0012.png', 'hand_0088.png', 'hand_0102.png', 'hand_0049.png']\n",
            "Testing set: ['hand_0157.png', 'hand_0104.png', 'hand_0020.png', 'hand_0117.png', 'hand_0052.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gya"
      ],
      "metadata": {
        "id": "h2tA-DWlDLcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"gya\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-gya\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "TlO7GAd3DMkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a5b16d-b091-48a1-8c67-e53a349a0baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 93\n",
            "Validation set size: 32\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0052.png', 'hand_0090.png', 'hand_0024.png', 'hand_0074.png', 'hand_0041.png']\n",
            "Validation set: ['hand_0152.png', 'hand_0010.png', 'hand_0053.png', 'hand_0038.png', 'hand_0086.png']\n",
            "Testing set: ['hand_0135.png', 'hand_0109.png', 'hand_0060.png', 'hand_0105.png', 'hand_0066.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ha"
      ],
      "metadata": {
        "id": "L88LFNNdDOSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "dpVBRvP2DMvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e07d240-d443-4e98-cd15-2abc31bd4b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 15\n",
            "Validation set size: 5\n",
            "Test set size: 6\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0053.png', 'hand_0094.png', 'hand_0083.png', 'hand_0048.png', 'hand_0027.png']\n",
            "Validation set: ['hand_0116.png', 'hand_0061.png', 'hand_0026.png', 'hand_0108.png', 'hand_0012.png']\n",
            "Testing set: ['hand_0050.png', 'hand_0001.png', 'hand_0091.png', 'hand_0034.png', 'hand_0090.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ja"
      ],
      "metadata": {
        "id": "0EhT5rzyDTGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ja\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ja\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "BgjviSMIDUVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4de2c4-27af-427a-92ca-3c6993c6fd1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 92\n",
            "Validation set size: 31\n",
            "Test set size: 31\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0116.png', 'hand_0082.png', 'hand_0102.png', 'hand_0064.png', 'hand_0119.png']\n",
            "Validation set: ['hand_0085.png', 'hand_0033.png', 'hand_0103.png', 'hand_0148.png', 'hand_0105.png']\n",
            "Testing set: ['hand_0088.png', 'hand_0131.png', 'hand_0097.png', 'hand_0045.png', 'hand_0001.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "jha"
      ],
      "metadata": {
        "id": "nYTHv8jwDVbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"jha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-jha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "9YF77vRQDWxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c007eb40-463f-4e4b-ef90-e04b4163614a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 85\n",
            "Validation set size: 29\n",
            "Test set size: 29\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0083.png', 'hand_0116.png', 'hand_0000.png', 'hand_0127.png', 'hand_0092.png']\n",
            "Validation set: ['hand_0052.png', 'hand_0128.png', 'hand_0047.png', 'hand_0142.png', 'hand_0084.png']\n",
            "Testing set: ['hand_0109.png', 'hand_0100.png', 'hand_0068.png', 'hand_0122.png', 'hand_0087.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ka"
      ],
      "metadata": {
        "id": "FCHk96LwDYM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ka\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ka\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "c91ZeccGDaYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2314c914-e254-4b26-d8c4-719d87b8cf47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 90\n",
            "Validation set size: 31\n",
            "Test set size: 31\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0027.png', 'hand_0051.png', 'hand_0110.png', 'hand_0103.png', 'hand_0017.png']\n",
            "Validation set: ['hand_0144.png', 'hand_0070.png', 'hand_0143.png', 'hand_0059.png', 'hand_0141.png']\n",
            "Testing set: ['hand_0091.png', 'hand_0146.png', 'hand_0063.png', 'hand_0134.png', 'hand_0035.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kha\n"
      ],
      "metadata": {
        "id": "P4xpMBK_De7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"kha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-kha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "kv4YmEtoDgr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9275b907-853a-47e7-e13d-07007c6d7e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 99\n",
            "Validation set size: 33\n",
            "Test set size: 33\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0148.png', 'hand_0085.png', 'hand_0010.png', 'hand_0151.png', 'hand_0055.png']\n",
            "Validation set: ['hand_0004.png', 'hand_0093.png', 'hand_0124.png', 'hand_0043.png', 'hand_0014.png']\n",
            "Testing set: ['hand_0039.png', 'hand_0135.png', 'hand_0053.png', 'hand_0073.png', 'hand_0147.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ksha\n"
      ],
      "metadata": {
        "id": "6o2VT7tODh25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ksha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ksha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "JtLQusDlDjqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db79d5f1-ec5c-4979-ee6b-da8b34fe90b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 87\n",
            "Validation set size: 29\n",
            "Test set size: 29\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0087.png', 'hand_0080.png', 'hand_0061.png', 'hand_0025.png', 'hand_0011.png']\n",
            "Validation set: ['hand_0024.png', 'hand_0096.png', 'hand_0048.png', 'hand_0092.png', 'hand_0069.png']\n",
            "Testing set: ['hand_0047.png', 'hand_0127.png', 'hand_0141.png', 'hand_0082.png', 'hand_0038.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "la"
      ],
      "metadata": {
        "id": "G4UNRbJ2Dk5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"la\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-la\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "KJzESkn4Dm3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039590a5-0b2c-49ec-d17c-a5e6ad5ace6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 92\n",
            "Validation set size: 31\n",
            "Test set size: 31\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0047.png', 'hand_0132.png', 'hand_0149.png', 'hand_0075.png', 'hand_0013.png']\n",
            "Validation set: ['hand_0092.png', 'hand_0145.png', 'hand_0056.png', 'hand_0129.png', 'hand_0131.png']\n",
            "Testing set: ['hand_0003.png', 'hand_0036.png', 'hand_0122.png', 'hand_0086.png', 'hand_0002.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "m_sha"
      ],
      "metadata": {
        "id": "x9Vs6XI_Do6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"m_sha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-m_sha-ष\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "o97EGa60DqLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1945707a-c5a7-46b2-e4b8-6a5aefbc5079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 82\n",
            "Validation set size: 28\n",
            "Test set size: 28\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0062.png', 'hand_0123.png', 'hand_0025.png', 'hand_0134.png', 'hand_0124.png']\n",
            "Validation set: ['hand_0022.png', 'hand_0087.png', 'hand_0023.png', 'hand_0048.png', 'hand_0054.png']\n",
            "Testing set: ['hand_0135.png', 'hand_0009.png', 'hand_0136.png', 'hand_0039.png', 'hand_0078.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ma"
      ],
      "metadata": {
        "id": "lspQFLv_Drer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ma\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ma\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "oDJuZhBVDsoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "647764c9-bfbe-41d3-abab-0eb111bedb99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 93\n",
            "Validation set size: 31\n",
            "Test set size: 31\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0070.png', 'hand_0052.png', 'hand_0149.png', 'hand_0120.png', 'hand_0049.png']\n",
            "Validation set: ['hand_0105.png', 'hand_0109.png', 'hand_0137.png', 'hand_0099.png', 'hand_0101.png']\n",
            "Testing set: ['hand_0106.png', 'hand_0031.png', 'hand_0037.png', 'hand_0107.png', 'hand_0033.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-na"
      ],
      "metadata": {
        "id": "oVUmauiTDt1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"na\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-na-ण\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "pHM8OAoKDv2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e61e76b-0cc8-43ed-9b8b-4ca368fdde00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 108\n",
            "Validation set size: 37\n",
            "Test set size: 37\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0106.png', 'hand_0035.png', 'hand_0112.png', 'hand_0067.png', 'hand_0063.png']\n",
            "Validation set: ['hand_0168.png', 'hand_0095.png', 'hand_0113.png', 'hand_0172.png', 'hand_0049.png']\n",
            "Testing set: ['hand_0122.png', 'hand_0084.png', 'hand_0129.png', 'hand_0083.png', 'hand_0061.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "naa"
      ],
      "metadata": {
        "id": "-zZs5_kZD4RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"naa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-naa-न\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "lFnVy--fD5TQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697f982d-cff1-4e21-dc95-609d3b53e552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 96\n",
            "Validation set size: 32\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0093.png', 'hand_0152.png', 'hand_0082.png', 'hand_0086.png', 'hand_0131.png']\n",
            "Validation set: ['hand_0140.png', 'hand_0156.png', 'hand_0022.png', 'hand_0039.png', 'hand_0000.png']\n",
            "Testing set: ['hand_0103.png', 'hand_0015.png', 'hand_0111.png', 'hand_0063.png', 'hand_0130.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nga"
      ],
      "metadata": {
        "id": "ri7bWtrBD74U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"nga\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-nga\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "FPFu1DyWD-FS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9b8afc-db49-4417-9c54-dcd401edc3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 105\n",
            "Validation set size: 35\n",
            "Test set size: 35\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0174.png', 'hand_0007.png', 'hand_0173.png', 'hand_0123.png', 'hand_0133.png']\n",
            "Validation set: ['hand_0066.png', 'hand_0072.png', 'hand_0110.png', 'hand_0087.png', 'hand_0118.png']\n",
            "Testing set: ['hand_0004.png', 'hand_0037.png', 'hand_0134.png', 'hand_0079.png', 'hand_0156.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pa"
      ],
      "metadata": {
        "id": "m0ok2rdND_Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"pa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-pa\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "f9j8yATAEAuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c32f8c-c712-4ea8-bca3-0fa0ac121bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 62\n",
            "Validation set size: 21\n",
            "Test set size: 21\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0034.png', 'hand_0094.png', 'hand_0013.png', 'hand_0029.png', 'hand_0075.png']\n",
            "Validation set: ['hand_0063.png', 'hand_0049.png', 'hand_0012.png', 'hand_0092.png', 'hand_0062.png']\n",
            "Testing set: ['hand_0037.png', 'hand_0016.png', 'hand_0036.png', 'hand_0059.png', 'hand_0067.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pha"
      ],
      "metadata": {
        "id": "vSOH49lqEB8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"pha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-pha\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "WlbOO85aEEBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3e6272-56ab-4cee-af0b-63aa5f636258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 87\n",
            "Validation set size: 29\n",
            "Test set size: 30\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0066.png', 'hand_0090.png', 'hand_0046.png', 'hand_0020.png', 'hand_0030.png']\n",
            "Validation set: ['hand_0086.png', 'hand_0029.png', 'hand_0056.png', 'hand_0143.png', 'hand_0054.png']\n",
            "Testing set: ['hand_0068.png', 'hand_0131.png', 'hand_0101.png', 'hand_0140.png', 'hand_0107.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ra"
      ],
      "metadata": {
        "id": "QmGGcydjEFfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ra\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ra\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "xYzGdxHaEIsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5787268-cd5a-4417-8e8a-83014f5c2050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 99\n",
            "Validation set size: 33\n",
            "Test set size: 33\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0068.png', 'hand_0101.png', 'hand_0047.png', 'hand_0072.png', 'hand_0128.png']\n",
            "Validation set: ['hand_0049.png', 'hand_0014.png', 'hand_0083.png', 'hand_0086.png', 'hand_0035.png']\n",
            "Testing set: ['hand_0152.png', 'hand_0043.png', 'hand_0015.png', 'hand_0130.png', 'hand_0144.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-t_sha"
      ],
      "metadata": {
        "id": "p7cTG4oPEJ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"t_sha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-t_sha-श\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "sdLhRFv7EOwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2adb3d-a498-43d6-e593-6fa63333ee38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 95\n",
            "Validation set size: 32\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0147.png', 'hand_0018.png', 'hand_0153.png', 'hand_0117.png', 'hand_0102.png']\n",
            "Validation set: ['hand_0011.png', 'hand_0105.png', 'hand_0036.png', 'hand_0068.png', 'hand_0049.png']\n",
            "Testing set: ['hand_0116.png', 'hand_0025.png', 'hand_0149.png', 'hand_0063.png', 'hand_0120.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ta"
      ],
      "metadata": {
        "id": "inSMVz60ERhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ta\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ta-ट\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "UrN5gRlHESd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354a4e73-7c79-49c8-93aa-85d87a96fa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 99\n",
            "Validation set size: 33\n",
            "Test set size: 33\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0115.png', 'hand_0023.png', 'hand_0057.png', 'hand_0022.png', 'hand_0087.png']\n",
            "Validation set: ['hand_0120.png', 'hand_0118.png', 'hand_0005.png', 'hand_0029.png', 'hand_0123.png']\n",
            "Testing set: ['hand_0158.png', 'hand_0135.png', 'hand_0071.png', 'hand_0036.png', 'hand_0077.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "taa"
      ],
      "metadata": {
        "id": "IYPaoisHEU9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"taa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-taa-त\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "9LRHYbNFEWFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340e39d0-ce4b-4daa-ac51-947b75bb2687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 114\n",
            "Validation set size: 38\n",
            "Test set size: 38\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0145.png', 'hand_0034.png', 'hand_0083.png', 'hand_0018.png', 'hand_0048.png']\n",
            "Validation set: ['hand_0156.png', 'hand_0094.png', 'hand_0121.png', 'hand_0046.png', 'hand_0167.png']\n",
            "Testing set: ['hand_0073.png', 'hand_0024.png', 'hand_0067.png', 'hand_0092.png', 'hand_0129.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tha"
      ],
      "metadata": {
        "id": "-5YiNHi8EXId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"tha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-tha-ठ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "JkvfM1rTEYz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0662599-9830-47d2-937d-11e11ca0b659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 102\n",
            "Validation set size: 35\n",
            "Test set size: 35\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0068.png', 'hand_0103.png', 'hand_0035.png', 'hand_0091.png', 'hand_0122.png']\n",
            "Validation set: ['hand_0098.png', 'hand_0120.png', 'hand_0084.png', 'hand_0011.png', 'hand_0044.png']\n",
            "Testing set: ['hand_0101.png', 'hand_0105.png', 'hand_0102.png', 'hand_0154.png', 'hand_0170.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "thaa"
      ],
      "metadata": {
        "id": "yUDZu-v3EZ0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"thaa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-thaa-थ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "YNT120vyEbL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e96f70c-f110-4526-bf36-7929fcf2434f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 102\n",
            "Validation set size: 34\n",
            "Test set size: 34\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0112.png', 'hand_0137.png', 'hand_0132.png', 'hand_0096.png', 'hand_0119.png']\n",
            "Validation set: ['hand_0123.png', 'hand_0156.png', 'hand_0039.png', 'hand_0111.png', 'hand_0167.png']\n",
            "Testing set: ['hand_0011.png', 'hand_0110.png', 'hand_0104.png', 'hand_0130.png', 'hand_0090.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tra\n"
      ],
      "metadata": {
        "id": "C_OD3vocEcWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"tra\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-tra\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "gdXKUuAFEdpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c983bb41-13ea-4ea8-ddba-c7650ec38256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 90\n",
            "Validation set size: 30\n",
            "Test set size: 30\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0146.png', 'hand_0141.png', 'hand_0109.png', 'hand_0038.png', 'hand_0002.png']\n",
            "Validation set: ['hand_0044.png', 'hand_0079.png', 'hand_0127.png', 'hand_0103.png', 'hand_0100.png']\n",
            "Testing set: ['hand_0084.png', 'hand_0102.png', 'hand_0052.png', 'hand_0058.png', 'hand_0045.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wa"
      ],
      "metadata": {
        "id": "JdLlTNUKEfGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"wa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-wa\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "KFn_jcx0EgWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701340e9-0402-4b63-a1c3-647c6803c525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 100\n",
            "Validation set size: 34\n",
            "Test set size: 34\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0024.png', 'hand_0164.png', 'hand_0106.png', 'hand_0148.png', 'hand_0081.png']\n",
            "Validation set: ['hand_0013.png', 'hand_0091.png', 'hand_0059.png', 'hand_0128.png', 'hand_0089.png']\n",
            "Testing set: ['hand_0102.png', 'hand_0060.png', 'hand_0099.png', 'hand_0022.png', 'hand_0034.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ya"
      ],
      "metadata": {
        "id": "qH_63NuFEhJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ya\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ya\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "dJI0hRscEjUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7060e7d-5f3c-4e01-c347-ab6ad9ba950f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 27\n",
            "Validation set size: 9\n",
            "Test set size: 9\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0006.png', 'hand_0013.png', 'hand_0039.png', 'hand_0005.png', 'hand_0027.png']\n",
            "Validation set: ['hand_0034.png', 'hand_0000.png', 'hand_0041.png', 'hand_0007.png', 'hand_0018.png']\n",
            "Testing set: ['hand_0014.png', 'hand_0043.png', 'hand_0026.png', 'hand_0021.png', 'hand_0017.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "yan"
      ],
      "metadata": {
        "id": "xp3XAA5gEnYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"yan\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-yan\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "5tL-YBgkEokF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4eab64-bc4b-4a47-b98c-6cb0c3541ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 80\n",
            "Validation set size: 27\n",
            "Test set size: 27\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0015.png', 'hand_0071.png', 'hand_0129.png', 'hand_0112.png', 'hand_0092.png']\n",
            "Validation set: ['hand_0133.png', 'hand_0113.png', 'hand_0076.png', 'hand_0050.png', 'hand_0080.png']\n",
            "Testing set: ['hand_0046.png', 'hand_0036.png', 'hand_0131.png', 'hand_0067.png', 'hand_0097.png']\n"
          ]
        }
      ]
    }
  ]
}